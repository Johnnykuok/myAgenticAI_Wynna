import json
import uuid
from datetime import datetime
from config import get_openai_client, DOUBAO_MODEL, SYSTEM_PROMPT
from conversation import save_conversation, load_conversation
from task_dispatcher import get_task_dispatcher, get_task_results
from task_summarizer import TaskSummarizer
from utils.timestamp_utils import get_current_timestamp
from utils.message_utils import create_user_message, create_assistant_message, create_system_message

def judge_question_type(user_message):
    """åˆ¤æ–­ç”¨æˆ·é—®é¢˜ç±»å‹ï¼šchatbotæ¨¡å¼ vs ä»»åŠ¡è§„åˆ’æ¨¡å¼"""
    try:
        client = get_openai_client()
        response = client.chat.completions.create(
            model=DOUBAO_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": """
                ä½ æ˜¯ä¸€ä¸ªé—®é¢˜ç±»å‹åˆ¤æ–­ä¸“å®¶ã€‚è¯·åˆ¤æ–­ç”¨æˆ·çš„é—®é¢˜å±äºä»¥ä¸‹å“ªç§ç±»å‹ï¼š
                1. chatBotæ¨¡å¼ï¼šç®€å•ä¸”å¸¸è§„çš„èŠå¤©ç±»é—®é¢˜ï¼Œå¦‚"ç°åœ¨å‡ ç‚¹äº†"ã€"æ­å·å¤©æ°”å¦‚ä½•"ã€"ä½ å«ä»€ä¹ˆåå­—"ã€"ä½ å¥½"ã€"å¸®æˆ‘å†™ä¸ªæ•…äº‹"ã€"è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½"ç­‰æ—¥å¸¸å¯¹è¯æˆ–ç®€å•å’¨è¯¢ã€‚
                2. taskPlanningæ¨¡å¼ï¼šå¤æ‚çš„ä»»åŠ¡è§„åˆ’ç±»é—®é¢˜ï¼Œéœ€è¦å¤šæ­¥éª¤å®Œæˆï¼Œå¦‚"ä¸ºæˆ‘åˆ¶å®š8æœˆä»½å»æ­å·çš„æ—…æ¸¸æ”»ç•¥"ã€"ä¸ºæˆ‘è°ƒç ”å¿«æ‰‹2024å¹´çš„è´¢æŠ¥"ã€"å¸®æˆ‘åˆ†æå¸‚åœºè¶‹åŠ¿å¹¶åˆ¶å®šå•†ä¸šè®¡åˆ’"ç­‰ã€‚
                
                è¯·ä»¥JSONæ ¼å¼è¾“å‡ºåˆ¤æ–­ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                {
                    "type": "chatBot",
                    "confidence": 0.95,
                    "reason": "è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ—¥å¸¸å’¨è¯¢é—®é¢˜"
                }
                
                æˆ–è€…ï¼š
                {
                    "type": "taskPlanning", 
                    "confidence": 0.88,
                    "reason": "è¿™æ˜¯ä¸€ä¸ªéœ€è¦å¤šæ­¥éª¤å®Œæˆçš„å¤æ‚ä»»åŠ¡"
                }
                
                å…¶ä¸­typeåªèƒ½æ˜¯"chatBot"æˆ–"taskPlanning"ï¼Œconfidenceä¸º0-1ä¹‹é—´çš„ç½®ä¿¡åº¦ï¼Œreasonä¸ºåˆ¤æ–­ç†ç”±ã€‚"""
                },
                {
                    "role": "user", 
                    "content": user_message
                }
            ],
            max_tokens=150,
            temperature=0.1,
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "question_type_judgment",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "type": {
                                "type": "string",
                                "enum": ["chatBot", "taskPlanning"],
                                "description": "é—®é¢˜ç±»å‹ï¼Œåªèƒ½æ˜¯chatBotæˆ–taskPlanning"
                            },
                            "confidence": {
                                "type": "number",
                                "description": "åˆ¤æ–­çš„ç½®ä¿¡åº¦ï¼ŒèŒƒå›´0-1"
                            },
                            "reason": {
                                "type": "string",
                                "description": "åˆ¤æ–­ç†ç”±"
                            }
                        },
                        "required": ["type", "confidence", "reason"],
                        "additionalProperties": False
                    },
                    "strict": True
                }
            }
        )
        
        result = json.loads(response.choices[0].message.content)
        question_type = result.get("type", "chatBot")
        confidence = result.get("confidence", 0.5)
        reason = result.get("reason", "")
        
        # è®°å½•åˆ¤æ–­ç»“æœ
        print(f"é—®é¢˜ç±»å‹åˆ¤æ–­: {question_type}, ç½®ä¿¡åº¦: {confidence}, ç†ç”±: {reason}")
        
        return question_type
            
    except Exception as e:
        print(f"åˆ¤æ–­é—®é¢˜ç±»å‹å¤±è´¥: {e}")
        # å¤±è´¥æ—¶é»˜è®¤ä¸ºchatBotæ¨¡å¼
        return "chatBot"

def decompose_task(user_message):
    """ä»»åŠ¡æ‹†è§£å‡½æ•°"""
    try:
        client = get_openai_client()
        response = client.chat.completions.create(
            model=DOUBAO_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": """
                    ä½ æ˜¯ä¸€ä¸ªå¤æ‚ä»»åŠ¡çš„æ‹†è§£ä¸“å®¶ï¼Œä½ ä¼šæŠŠç”¨æˆ·çš„é—®é¢˜æ‹†è§£ä¸ºå°çš„æ­¥éª¤ã€‚

                    è¯·ä»¥JSONæ ¼å¼è¾“å‡ºæ‹†è§£ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                    {
                        "tasks": [
                            "æ­¥éª¤ä¸€çš„å…·ä½“æè¿°",
                            "æ­¥éª¤äºŒçš„å…·ä½“æè¿°",
                            "æ­¥éª¤ä¸‰çš„å…·ä½“æè¿°"
                        ],
                        "markdown": "# TODO\n\n1. æ­¥éª¤ä¸€çš„å…·ä½“æè¿°\n2. æ­¥éª¤äºŒçš„å…·ä½“æè¿°\n3. æ­¥éª¤ä¸‰çš„å…·ä½“æè¿°"
                    }
                    
                    å…¶ä¸­tasksä¸ºæ‹†è§£åçš„ä»»åŠ¡åˆ—è¡¨ï¼Œmarkdownä¸ºto_do.mdæ ¼å¼çš„å†…å®¹ã€‚"""
                },
                {
                    "role": "user", 
                    "content": user_message
                }
            ],
            max_tokens=500,
            temperature=0.3,
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "task_decomposition",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "tasks": {
                                "type": "array",
                                "items": {
                                    "type": "string",
                                    "description": "å…·ä½“çš„ä»»åŠ¡æ­¥éª¤æè¿°"
                                },
                                "description": "æ‹†è§£åçš„ä»»åŠ¡æ­¥éª¤åˆ—è¡¨"
                            },
                            "markdown": {
                                "type": "string",
                                "description": "TODOæ ¼å¼çš„markdownå†…å®¹"
                            }
                        },
                        "required": ["tasks", "markdown"],
                        "additionalProperties": False
                    },
                    "strict": True
                }
            }
        )
        
        result = json.loads(response.choices[0].message.content)
        tasks = result.get("tasks", [])
        markdown = result.get("markdown", "")
        
        # è®°å½•æ‹†è§£ç»“æœ
        print(f"ä»»åŠ¡æ‹†è§£æˆåŠŸï¼Œå…±{len(tasks)}ä¸ªæ­¥éª¤")
        
        # è¿”å›markdownæ ¼å¼ï¼Œä¿æŒä¸åŸæœ‰ä»£ç å…¼å®¹
        return markdown
        
    except Exception as e:
        print(f"ä»»åŠ¡æ‹†è§£å¤±è´¥: {e}")
        return f"ä»»åŠ¡æ‹†è§£å¤±è´¥ï¼š{str(e)}"


def handle_task_planning(user_input, conversation_id=None):
    """å¤„ç†ä»»åŠ¡è§„åˆ’æ¨¡å¼"""
    if conversation_id is None:
        conversation_id = str(uuid.uuid4())
    
    # ä»»åŠ¡æ‹†è§£
    decomposed_tasks = decompose_task(user_input)
    
    # ä¿å­˜åˆå§‹å¯¹è¯ï¼ˆä½¿ç”¨å·¥å…·å‡½æ•°åˆ›å»ºæ¶ˆæ¯ï¼‰
    messages = [
        create_system_message(SYSTEM_PROMPT),
        create_user_message(user_input),
        create_assistant_message(f"æˆ‘æ¥å¸®ä½ åˆ†æè¿™ä¸ªä»»åŠ¡ï½è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¤æ‚çš„é—®é¢˜ï¼Œæˆ‘æŠŠå®ƒæ‹†è§£æˆäº†ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š\n\n{decomposed_tasks}\n\nè¯·ç¡®è®¤è¿™äº›æ­¥éª¤æ˜¯å¦åˆé€‚ï¼Œæˆ–è€…ä½ å¯ä»¥ç¼–è¾‘åæäº¤ã€‚ç¡®è®¤åæˆ‘ä¼šé€æ­¥ä¸ºä½ å®Œæˆæ¯ä¸ªä»»åŠ¡å“¦ï¼")
    ]
    
    save_conversation(conversation_id, messages, mode="taskPlanning")
    
    return {
        "response": f"æˆ‘æ¥å¸®ä½ åˆ†æè¿™ä¸ªä»»åŠ¡ï½è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¤æ‚çš„é—®é¢˜ï¼Œæˆ‘æŠŠå®ƒæ‹†è§£æˆäº†ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š\n\n{decomposed_tasks}\n\nè¯·ç¡®è®¤è¿™äº›æ­¥éª¤æ˜¯å¦åˆé€‚ï¼Œæˆ–è€…ä½ å¯ä»¥ç¼–è¾‘åæäº¤ã€‚ç¡®è®¤åæˆ‘ä¼šé€æ­¥ä¸ºä½ å®Œæˆæ¯ä¸ªä»»åŠ¡å“¦ï¼",
        "conversation_id": conversation_id,
        "mode": "taskPlanning",
        "decomposed_tasks": decomposed_tasks,
        "original_question": user_input,
        "status": "waiting_confirmation"
    }

async def confirm_and_execute_tasks_new(conversation_id, confirmed_tasks, original_question, modified_todo_content=None):
    """ä½¿ç”¨æ–°çš„ä»»åŠ¡åˆ†é…å™¨ç¡®è®¤å¹¶æ‰§è¡Œä»»åŠ¡"""
    try:
        # é‡æ„ä»»åŠ¡ä¸ºmarkdownæ ¼å¼
        todo_content = "# TODO\n\n"
        for i, task in enumerate(confirmed_tasks, 1):
            todo_content += f"{i}. {task}\n"
        
        print(f"ğŸ“‹ å¼€å§‹æ‰§è¡Œ {len(confirmed_tasks)} ä¸ªä»»åŠ¡")
        
        # è·å–ä»»åŠ¡åˆ†é…å™¨å¹¶æ‰§è¡Œä»»åŠ¡
        dispatcher = await get_task_dispatcher()
        cache_key = await dispatcher.dispatch_and_execute_tasks(original_question, todo_content)
        
        print(f"âœ… æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œç¼“å­˜é”®: {cache_key}")
        
        # è·å–æ‰§è¡Œç»“æœ
        cache_data = get_task_results(cache_key)
        if not cache_data:
            raise Exception("æ— æ³•è·å–ä»»åŠ¡æ‰§è¡Œç»“æœ")
        
        # ä½¿ç”¨ä»»åŠ¡æ±‡æ€»å™¨ç”Ÿæˆæœ€ç»ˆå“åº”
        summarizer = TaskSummarizer()
        final_response = summarizer.generate_final_response(cache_data)
        
        # æ›´æ–°å¯¹è¯è®°å½•ï¼ˆä½¿ç”¨å·¥å…·å‡½æ•°åˆ›å»ºæ¶ˆæ¯ï¼‰
        messages = load_conversation(conversation_id)
        
        # ç›´æ¥ä½¿ç”¨ç”¨æˆ·ä¿®æ”¹åçš„todoå†…å®¹ï¼Œä¸æ·»åŠ å‰ç¼€
        if modified_todo_content:
            user_confirmation_content = modified_todo_content
        else:
            # å¦‚æœæ²¡æœ‰ä¿®æ”¹åçš„å†…å®¹ï¼Œé‡æ„ä¸ºmarkdownæ ¼å¼
            user_confirmation_content = "# TODO\n\n"
            for i, task in enumerate(confirmed_tasks, 1):
                user_confirmation_content += f"{i}. {task}\n"
        
        messages.append(create_user_message(user_confirmation_content))
        messages.append(create_assistant_message(final_response["response"]))
        save_conversation(conversation_id, messages)
        
        # æ·»åŠ å¯¹è¯IDåˆ°å“åº”
        final_response["conversation_id"] = conversation_id
        
        return final_response
        
    except Exception as e:
        print(f"æ‰§è¡Œä»»åŠ¡å¤±è´¥: {e}")
        return {
            "response": f"æ‰§è¡Œä»»åŠ¡æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
            "conversation_id": conversation_id,
            "mode": "taskPlanning",
            "status": "error"
        }

